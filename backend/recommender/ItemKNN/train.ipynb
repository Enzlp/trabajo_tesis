{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4c9f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instalacion librerias\n",
    "!pip install implicit -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235956e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from implicit.nearest_neighbours import CosineRecommender\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# 1. Cargar datos y artefactos\n",
    "# -----------------------\n",
    "# Cargamos los datos crudos y diccionarios de mapeo\n",
    "df_pairs_unique = pd.read_pickle(\"data/df_pairs_unique.pkl\")\n",
    "author2idx = np.load(\"data/author_to_idx.npy\", allow_pickle=True).item()\n",
    "idx2author = np.load(\"data/idx_to_author.npy\", allow_pickle=True).item()\n",
    "n_authors = len(author2idx)\n",
    "\n",
    "# Filtrar autores con >= 2 colaboraciones para permitir LOO riguroso\n",
    "author_counts = df_pairs_unique[['pair_min', 'pair_max']].stack().value_counts()\n",
    "eligible_authors = set(author_counts[author_counts >= 2].index)\n",
    "\n",
    "df_filtered = df_pairs_unique[\n",
    "    df_pairs_unique['pair_min'].isin(eligible_authors) &\n",
    "    df_pairs_unique['pair_max'].isin(eligible_authors)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Autores elegibles (>=2): {len(eligible_authors):,}\")\n",
    "print(f\"Pares tras filtrado: {len(df_filtered):,}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2. Split LOO Triple (Train/Val/Test)\n",
    "# -----------------------\n",
    "def triple_loo_split(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    adj = defaultdict(list)\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        adj[row.pair_min].append(idx)\n",
    "        adj[row.pair_max].append(idx)\n",
    "\n",
    "    test_idx = set()\n",
    "    val_idx = set()\n",
    "    assigned = set()\n",
    "\n",
    "    # Selección aleatoria para Validación y Test\n",
    "    for author in adj:\n",
    "        # Solo consideramos índices que no hayan sido asignados a otro autor aún\n",
    "        indices = [i for i in adj[author] if i not in assigned]\n",
    "        if len(indices) >= 3:\n",
    "            rng.shuffle(indices)\n",
    "            test_idx.add(indices[0])\n",
    "            val_idx.add(indices[1])\n",
    "            assigned.update([indices[0], indices[1]])\n",
    "        elif len(indices) == 2:\n",
    "            t = rng.choice(indices)\n",
    "            test_idx.add(t)\n",
    "            assigned.add(t)\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    train_idx = np.setdiff1d(all_indices, list(test_idx | val_idx))\n",
    "\n",
    "    return (\n",
    "        df.iloc[train_idx].reset_index(drop=True),\n",
    "        df.iloc[list(val_idx)].reset_index(drop=True),\n",
    "        df.iloc[list(test_idx)].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(\"Generando splits...\")\n",
    "df_train, df_val, df_test = triple_loo_split(df_filtered)\n",
    "\n",
    "# -----------------------\n",
    "# 3. Funciones de Métricas y Recomendación\n",
    "# -----------------------\n",
    "def build_csr(df, author2idx, n):\n",
    "    u_idx = df['pair_min'].map(author2idx).values\n",
    "    v_idx = df['pair_max'].map(author2idx).values\n",
    "    rows = np.concatenate([u_idx, v_idx])\n",
    "    cols = np.concatenate([v_idx, u_idx])\n",
    "    data = np.ones(len(rows), dtype=np.float32)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "\n",
    "def build_gt(df):\n",
    "    gt = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        gt[row.pair_min].add(row.pair_max)\n",
    "        gt[row.pair_max].add(row.pair_min)\n",
    "    return gt\n",
    "\n",
    "def recall_ndcg_at_k(recs, gt, k=20):\n",
    "    r_sum, n_sum, count = 0.0, 0.0, 0\n",
    "    for u, rel in gt.items():\n",
    "        if not rel: continue\n",
    "        ranked = recs.get(u, [])[:k]\n",
    "        hits = [1 if i in rel else 0 for i in ranked]\n",
    "        r_sum += sum(hits) / len(rel)\n",
    "        dcg = sum(h / np.log2(i + 2) for i, h in enumerate(hits))\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(rel), k)))\n",
    "        n_sum += (dcg / idcg) if idcg > 0 else 0\n",
    "        count += 1\n",
    "    return (r_sum / count, n_sum / count) if count > 0 else (0, 0)\n",
    "\n",
    "def calculate_coverage(recs, all_possible_items):\n",
    "    recommended_items = set()\n",
    "    for items in recs.values():\n",
    "        recommended_items.update(items)\n",
    "    return len(recommended_items) / len(all_possible_items)\n",
    "\n",
    "def calculate_novelty(recs, train_df, k=20):\n",
    "    # Popularidad basada en interacciones de entrenamiento\n",
    "    item_counts = train_df[['pair_min', 'pair_max']].stack().value_counts().to_dict()\n",
    "    total_interactions = sum(item_counts.values())\n",
    "\n",
    "    nov_sum, count = 0.0, 0\n",
    "    for u, ranked_list in recs.items():\n",
    "        if not ranked_list: continue\n",
    "        # Self-information: -log2(p(i))\n",
    "        u_nov = sum(-np.log2(item_counts.get(it, 1) / total_interactions) for it in ranked_list[:k])\n",
    "        nov_sum += (u_nov / k)\n",
    "        count += 1\n",
    "    return nov_sum / count if count > 0 else 0\n",
    "\n",
    "def get_recommendations(model, train_matrix, target_authors, topk=20):\n",
    "    recs = {}\n",
    "    train_matrix = train_matrix.tocsr().astype(np.float32)\n",
    "    for author in tqdm(target_authors, desc=\"Recomendando\", leave=False):\n",
    "        u_idx = author2idx[author]\n",
    "        ids, _ = model.recommend(userid=int(u_idx), user_items=train_matrix[u_idx],\n",
    "                                 N=topk, filter_already_liked_items=True)\n",
    "        recs[author] = [idx2author[x] for x in ids]\n",
    "    return recs\n",
    "\n",
    "# -----------------------\n",
    "# 4. Afinamiento de Hiperparámetros (Validation)\n",
    "# -----------------------\n",
    "X_train_v = build_csr(df_train, author2idx, n_authors)\n",
    "gt_val = build_gt(df_val)\n",
    "val_authors = list(gt_val.keys())\n",
    "val_sample = list(np.random.default_rng(42).choice(val_authors, size=min(20000, len(val_authors)), replace=False))\n",
    "\n",
    "best_k, best_ndcg = 0, -1\n",
    "ks_to_test = [20, 50, 100, 150, 200]\n",
    "\n",
    "print(\"\\n--- TUNING: RECALL & NDCG ---\")\n",
    "for k in ks_to_test:\n",
    "    model = CosineRecommender(K=k)\n",
    "    model.fit(X_train_v, show_progress=False)\n",
    "    recs = get_recommendations(model, X_train_v, val_sample)\n",
    "    rec, ndcg = recall_ndcg_at_k(recs, {a: gt_val[a] for a in val_sample})\n",
    "    print(f\"K={k:3} | Recall@20: {rec:.4f} | NDCG@20: {ndcg:.4f}\")\n",
    "    if ndcg > best_ndcg:\n",
    "        best_ndcg, best_k = ndcg, k\n",
    "\n",
    "# -----------------------\n",
    "# 5. Evaluación Final (Test)\n",
    "# -----------------------\n",
    "print(f\"\\n--- EVALUACIÓN FINAL (K={best_k}) ---\")\n",
    "# Unimos Train + Val para el entrenamiento final antes de test\n",
    "df_final_train = pd.concat([df_train, df_val]).reset_index(drop=True)\n",
    "X_final_train = build_csr(df_final_train, author2idx, n_authors)\n",
    "gt_test = build_gt(df_test)\n",
    "test_authors = list(gt_test.keys())\n",
    "\n",
    "final_model = CosineRecommender(K=best_k)\n",
    "final_model.fit(X_final_train, show_progress=True)\n",
    "\n",
    "# Generar recomendaciones para todos los autores en Test\n",
    "recs_test = get_recommendations(final_model, X_final_train, test_authors)\n",
    "\n",
    "# Métricas Finales\n",
    "f_rec, f_ndcg = recall_ndcg_at_k(recs_test, gt_test)\n",
    "f_cov = calculate_coverage(recs_test, set(df_final_train['pair_min']) | set(df_final_train['pair_max']))\n",
    "f_nov = calculate_novelty(recs_test, df_final_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"RESULTADOS FINALES (LOO)\")\n",
    "print(f\"Muestra Test: {len(test_authors):,} autores\")\n",
    "print(f\"Recall@20:   {f_rec:.4f}\")\n",
    "print(f\"NDCG@20:     {f_ndcg:.4f}\")\n",
    "print(f\"Coverage:    {f_cov:.4f}\")\n",
    "print(f\"Novelty:     {f_nov:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# -----------------------\n",
    "# 6. Guardado y Producción\n",
    "# -----------------------\n",
    "X_full = build_csr(df_pairs_unique, author2idx, n_authors)\n",
    "final_model.fit(X_full, show_progress=True)\n",
    "\n",
    "folder = \"models_final\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "final_model.save(f\"{folder}/itemknn_best.npz\")\n",
    "save_npz(f\"{folder}/X_full.npz\", X_full)\n",
    "print(f\"Proceso finalizado. Modelo guardado en {folder}/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
