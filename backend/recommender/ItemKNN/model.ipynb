{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d159df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.nearest_neighbours import CosineRecommender\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# 1. Cargar datos\n",
    "# -----------------------\n",
    "df_pairs_unique = pd.read_pickle(\"data/df_pairs_unique.pkl\")\n",
    "author2idx = np.load(\"data/author_to_idx.npy\", allow_pickle=True).item()\n",
    "idx2author = np.load(\"data/idx_to_author.npy\", allow_pickle=True).item()\n",
    "n_authors = len(author2idx)\n",
    "\n",
    "print(f\"Autores: {n_authors:,}\")\n",
    "print(f\"Pares únicos: {len(df_pairs_unique):,}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2. Split LOO (Optimizado)\n",
    "# -----------------------\n",
    "def train_val_test_split(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Agrupar por autor usando diccionarios de listas de índices\n",
    "    adj = defaultdict(list)\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        adj[row.pair_min].append(idx)\n",
    "        adj[row.pair_max].append(idx)\n",
    "\n",
    "    test_idx = set()\n",
    "    val_idx = set()\n",
    "    assigned = set()\n",
    "\n",
    "    # Selección aleatoria para LOO\n",
    "    for author, indices in adj.items():\n",
    "        available = [i for i in indices if i not in assigned]\n",
    "        if len(available) >= 3:\n",
    "            # Shuffle local para elegir\n",
    "            rng.shuffle(available)\n",
    "            t = available[0]\n",
    "            v = available[1]\n",
    "            test_idx.add(t)\n",
    "            val_idx.add(v)\n",
    "            assigned.update([t, v])\n",
    "        elif len(available) == 2:\n",
    "            t = rng.choice(available)\n",
    "            test_idx.add(t)\n",
    "            assigned.add(t)\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    train_idx = np.setdiff1d(all_indices, list(test_idx | val_idx))\n",
    "\n",
    "    return (\n",
    "        df.iloc[train_idx].reset_index(drop=True),\n",
    "        df.iloc[list(val_idx)].reset_index(drop=True),\n",
    "        df.iloc[list(test_idx)].reset_index(drop=True),\n",
    "    )\n",
    "\n",
    "df_train, df_val, df_test = train_val_test_split(df_pairs_unique)\n",
    "\n",
    "# -----------------------\n",
    "# 3. Construcción CSR Binaria (Corregida con Dtypes)\n",
    "# -----------------------\n",
    "def build_csr(df, author2idx, n):\n",
    "    # Usar arrays de numpy directamente es mucho más rápido y seguro para dtypes\n",
    "    u_indices = df['pair_min'].map(author2idx).values\n",
    "    v_indices = df['pair_max'].map(author2idx).values\n",
    "    \n",
    "    # Grafo no dirigido: duplicamos las conexiones\n",
    "    rows = np.concatenate([u_indices, v_indices])\n",
    "    cols = np.concatenate([v_indices, u_indices])\n",
    "    # Implicit prefiere float32 para los datos de la matriz\n",
    "    data = np.ones(len(rows), dtype=np.float32)\n",
    "    \n",
    "    return csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "\n",
    "X_train = build_csr(df_train, author2idx, n_authors)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Ground Truth\n",
    "# -----------------------\n",
    "def build_gt(df):\n",
    "    gt = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        gt[row.pair_min].add(row.pair_max)\n",
    "        gt[row.pair_max].add(row.pair_min)\n",
    "    return gt\n",
    "\n",
    "gt_val = build_gt(df_val)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Métricas\n",
    "# -----------------------\n",
    "def recall_ndcg_at_k(recs, gt, k=20):\n",
    "    r_sum, n_sum, count = 0.0, 0.0, 0\n",
    "    for u, rel in gt.items():\n",
    "        if not rel: continue\n",
    "        ranked = recs.get(u, [])[:k]\n",
    "        \n",
    "        hits = [1 if i in rel else 0 for i in ranked]\n",
    "        # Recall\n",
    "        r_sum += sum(hits) / len(rel)\n",
    "        # NDCG\n",
    "        dcg = sum(h / np.log2(i + 2) for i, h in enumerate(hits))\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(rel), k)))\n",
    "        n_sum += (dcg / idcg) if idcg > 0 else 0\n",
    "        count += 1\n",
    "    return r_sum / count, n_sum / count\n",
    "\n",
    "# -----------------------\n",
    "# 6. Recomendación (Solución al TypeError)\n",
    "# -----------------------\n",
    "def get_recommendations(model, train_matrix, target_authors, author2idx, idx2author, topk=20):\n",
    "    recs = {}\n",
    "    # Aseguramos que la matriz sea CSR y float32 antes de entrar al bucle\n",
    "    train_matrix = train_matrix.tocsr().astype(np.float32)\n",
    "    \n",
    "    for author in tqdm(target_authors, desc=\"Recomendando\", leave=False):\n",
    "        u_idx = author2idx[author]\n",
    "        \n",
    "        # El error \"No matching signature\" ocurre por el tipo de datos en u_idx o train_matrix\n",
    "        ids, scores = model.recommend(\n",
    "            userid=u_idx, \n",
    "            user_items=train_matrix[u_idx], \n",
    "            N=topk, \n",
    "            filter_already_liked_items=True\n",
    "        )\n",
    "        recs[author] = [idx2author[x] for x in ids]\n",
    "    return recs\n",
    "\n",
    "# -----------------------\n",
    "# 7. Grid Search\n",
    "# -----------------------\n",
    "sample_authors = list(gt_val.keys())\n",
    "if len(sample_authors) > 20000:\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_authors = list(rng.choice(sample_authors, size=20000, replace=False))\n",
    "\n",
    "gt_val_sample = {a: gt_val[a] for a in sample_authors}\n",
    "\n",
    "results = []\n",
    "ks_to_test = [20, 50, 100, 150, 200]\n",
    "\n",
    "print(\"\\nIniciando Grid Search...\")\n",
    "for k in ks_to_test:\n",
    "    model = CosineRecommender(K=k)\n",
    "    # Fit requiere que la matriz sea CSR\n",
    "    model.fit(X_train, show_progress=False)\n",
    "    \n",
    "    recs = get_recommendations(model, X_train, sample_authors, author2idx, idx2author, topk=20)\n",
    "    recall, ndcg = recall_ndcg_at_k(recs, gt_val_sample, k=20)\n",
    "    \n",
    "    results.append((k, recall, ndcg))\n",
    "    print(f\"K={k} | Recall@20: {recall:.4f} | NDCG@20: {ndcg:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8. Guardar\n",
    "# -----------------------\n",
    "df_results = pd.DataFrame(results, columns=[\"k\", \"recall@20\", \"ndcg@20\"])\n",
    "df_results.to_csv(\"knn_grid_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6de87d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCRIPT ITEMKNN CON LOO CLÁSICO — Evaluación FINAL\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "from implicit.nearest_neighbours import CosineRecommender\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# 1. Cargar artefactos y el mejor K\n",
    "# -----------------------\n",
    "best_k = 200\n",
    "df_pairs_unique = pd.read_pickle(\"data/df_pairs_unique.pkl\")\n",
    "author2idx = np.load(\"data/author_to_idx.npy\", allow_pickle=True).item()\n",
    "idx2author = np.load(\"data/idx_to_author.npy\", allow_pickle=True).item()\n",
    "n_authors = len(author2idx)\n",
    "\n",
    "print(f\"Evaluación final con hiperparámetro k={best_k}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2. Filtrar autores elegibles para LOO clásico (≥2 colaboraciones)\n",
    "# -----------------------\n",
    "author_counts = df_pairs_unique[['pair_min','pair_max']].stack().value_counts()\n",
    "eligible_authors = author_counts[author_counts >= 2].index\n",
    "\n",
    "df_test_candidates = df_pairs_unique[\n",
    "    df_pairs_unique['pair_min'].isin(eligible_authors) &\n",
    "    df_pairs_unique['pair_max'].isin(eligible_authors)\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# 3. Split LOO clásico\n",
    "# -----------------------\n",
    "def train_test_split_LOO(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    adj = defaultdict(list)\n",
    "    \n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        adj[row.pair_min].append(idx)\n",
    "        adj[row.pair_max].append(idx)\n",
    "\n",
    "    test_idx = set()\n",
    "    for author, indices in adj.items():\n",
    "        if len(indices) < 2:\n",
    "            continue\n",
    "        t = rng.choice(indices)\n",
    "        test_idx.add(t)\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    train_idx = np.setdiff1d(all_indices, list(test_idx))\n",
    "\n",
    "    return df.iloc[train_idx].reset_index(drop=True), df.iloc[list(test_idx)].reset_index(drop=True)\n",
    "\n",
    "df_train, df_test = train_test_split_LOO(df_test_candidates)\n",
    "\n",
    "print(f\"Split final: {len(df_train)} train / {len(df_test)} test\")\n",
    "\n",
    "# -----------------------\n",
    "# 4. Construcción CSR Binaria\n",
    "# -----------------------\n",
    "def build_csr(df, author2idx, n):\n",
    "    u_idx = df['pair_min'].map(author2idx).values\n",
    "    v_idx = df['pair_max'].map(author2idx).values\n",
    "    rows = np.concatenate([u_idx, v_idx])\n",
    "    cols = np.concatenate([v_idx, u_idx])\n",
    "    data = np.ones(len(rows), dtype=np.float32)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "\n",
    "X_train = build_csr(df_train, author2idx, n_authors)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Ground Truth Test\n",
    "# -----------------------\n",
    "def build_gt(df):\n",
    "    gt = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        gt[row.pair_min].add(row.pair_max)\n",
    "        gt[row.pair_max].add(row.pair_min)\n",
    "    return gt\n",
    "\n",
    "gt_test = build_gt(df_test)\n",
    "\n",
    "# -----------------------\n",
    "# 6. Métricas Recall/NDCG\n",
    "# -----------------------\n",
    "def recall_ndcg_at_k(recs, gt, k=20):\n",
    "    r_sum, n_sum, count = 0.0, 0.0, 0\n",
    "    for u, rel in gt.items():\n",
    "        if not rel: continue\n",
    "        ranked = recs.get(u, [])[:k]\n",
    "        hits = [1 if i in rel else 0 for i in ranked]\n",
    "        r_sum += sum(hits) / len(rel)\n",
    "        dcg = sum(h / np.log2(i + 2) for i, h in enumerate(hits))\n",
    "        idcg = sum(1 / np.log2(i + 2) for i in range(min(len(rel), k)))\n",
    "        n_sum += (dcg / idcg) if idcg > 0 else 0\n",
    "        count += 1\n",
    "    return (r_sum / count, n_sum / count) if count > 0 else (0, 0)\n",
    "\n",
    "# -----------------------\n",
    "# 7. Entrenamiento modelo final LOO clásico\n",
    "# -----------------------\n",
    "model = CosineRecommender(K=best_k)\n",
    "print(\"Entrenando modelo final LOO clásico...\")\n",
    "model.fit(X_train.astype(np.float32), show_progress=True)\n",
    "\n",
    "# -----------------------\n",
    "# 8. Generación de recomendaciones para test\n",
    "# -----------------------\n",
    "def get_final_recommendations(model, train_matrix, target_authors, author2idx, idx2author, topk=20):\n",
    "    final_recs = {}\n",
    "    train_matrix = train_matrix.tocsr().astype(np.float32)\n",
    "    for author in tqdm(target_authors, desc=\"Recomendando para Test\"):\n",
    "        u_idx = author2idx[author]\n",
    "        ids, scores = model.recommend(\n",
    "            userid=int(u_idx),\n",
    "            user_items=train_matrix[u_idx],\n",
    "            N=topk,\n",
    "            filter_already_liked_items=True\n",
    "        )\n",
    "        final_recs[author] = [idx2author[x] for x in ids]\n",
    "    return final_recs\n",
    "\n",
    "target_authors = list(gt_test.keys())\n",
    "recs = get_final_recommendations(model, X_train, target_authors, author2idx, idx2author)\n",
    "\n",
    "recall, ndcg = recall_ndcg_at_k(recs, gt_test, k=20)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"RESULTADOS FINALES LOO CLÁSICO (K={best_k})\")\n",
    "print(f\"Muestra de evaluación: {len(target_authors):,} autores\")\n",
    "print(f\"Recall@20 = {recall:.4f}\")\n",
    "print(f\"NDCG@20  = {ndcg:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# -----------------------\n",
    "# 9. Entrenamiento final para producción (todos los autores)\n",
    "# -----------------------\n",
    "X_train_full = build_csr(df_pairs_unique, author2idx, n_authors)\n",
    "model.fit(X_train_full.astype(np.float32), show_progress=True)\n",
    "\n",
    "# -----------------------\n",
    "# 10. Guardar artefactos\n",
    "# -----------------------\n",
    "folder = \"models_final\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "model.save(f\"{folder}/itemknn_model_full.npz\")\n",
    "save_npz(f\"{folder}/X_train_final.npz\", X_train_full.astype(np.float32))\n",
    "\n",
    "print(\"¡Todo guardado correctamente!\")\n",
    "print(f\"Archivos: {os.listdir(folder)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
