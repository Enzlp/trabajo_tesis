{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380ad4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch-geometric -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790ce79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import LightGCN\n",
    "from torch_geometric.utils import remove_self_loops, to_undirected\n",
    "\n",
    "# ===============================================================\n",
    "# 0. CONFIGURACIN OPTIMIZADA (40GB VRAM)\n",
    "# ===============================================================\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "K_METRIC = 20\n",
    "SAVE_DIR = \"results_lightgcn_40gb\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ===============================================================\n",
    "# 1. PREPARACIN DE DATOS Y POPULARIDAD\n",
    "# ===============================================================\n",
    "def preprocess_data(df_pairs):\n",
    "    print(\" Filtrando autores con >= 2 colaboraciones...\")\n",
    "    counts = pd.concat([df_pairs['pair_min'], df_pairs['pair_max']]).astype(str).value_counts()\n",
    "    eligible_authors = counts[counts >= 2].index\n",
    "    df_filtered = df_pairs[\n",
    "        df_pairs['pair_min'].astype(str).isin(eligible_authors) &\n",
    "        df_pairs['pair_max'].astype(str).isin(eligible_authors)\n",
    "    ].copy()\n",
    "    return df_filtered\n",
    "\n",
    "def get_item_popularity_tensor(df_train, author_to_idx, n_authors):\n",
    "    pop_counts = pd.concat([df_train['pair_min'], df_train['pair_max']]).astype(str).value_counts()\n",
    "    total_interactions = len(df_train) * 2\n",
    "    popularity_array = np.ones(n_authors)\n",
    "    for auth_id, count in pop_counts.items():\n",
    "        if auth_id in author_to_idx:\n",
    "            popularity_array[author_to_idx[auth_id]] = count\n",
    "    novelty_scores = -np.log2(popularity_array / total_interactions)\n",
    "    return torch.tensor(novelty_scores, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "def triple_loo_split(df, seed=42):\n",
    "    print(\"锔 Generando Split LOO...\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    adj = defaultdict(list)\n",
    "    for idx, row in enumerate(df.itertuples()):\n",
    "        u, v = str(row.pair_min), str(row.pair_max)\n",
    "        adj[u].append(idx); adj[v].append(idx)\n",
    "\n",
    "    test_idx, val_idx, assigned = set(), set(), set()\n",
    "    for author in adj:\n",
    "        indices = [i for i in adj[author] if i not in assigned]\n",
    "        if len(indices) >= 3:\n",
    "            rng.shuffle(indices)\n",
    "            test_idx.add(indices[0]); val_idx.add(indices[1])\n",
    "            assigned.update([indices[0], indices[1]])\n",
    "        elif len(indices) == 2:\n",
    "            t = rng.choice(indices)\n",
    "            test_idx.add(t); assigned.add(t)\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    train_idx = np.setdiff1d(all_indices, list(test_idx | val_idx))\n",
    "    return df.iloc[train_idx], df.iloc[list(val_idx)], df.iloc[list(test_idx)]\n",
    "\n",
    "# ===============================================================\n",
    "# 2. UTILIDADES DE GRAFO\n",
    "# ===============================================================\n",
    "def build_edge_index(df, mapping):\n",
    "    u = df[\"pair_min\"].astype(str).map(mapping).values\n",
    "    v = df[\"pair_max\"].astype(str).map(mapping).values\n",
    "    edge_index = torch.tensor(np.array([u, v]), dtype=torch.long)\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    edge_index, _ = remove_self_loops(edge_index)\n",
    "    return edge_index.to(DEVICE)\n",
    "\n",
    "def build_adj_dict(df, mapping):\n",
    "    adj = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        u, v = mapping[str(row.pair_min)], mapping[str(row.pair_max)]\n",
    "        adj[u].add(v); adj[v].add(u)\n",
    "    return adj\n",
    "\n",
    "def sample_batch_fast(adj_list, keys, num_nodes, batch_size):\n",
    "    u = np.random.choice(keys, size=batch_size, replace=True)\n",
    "    p = np.array([random.choice(adj_list[user]) for user in u])\n",
    "    n = np.random.randint(0, num_nodes, size=batch_size)\n",
    "    return (torch.tensor(u, device=DEVICE), torch.tensor(p, device=DEVICE), torch.tensor(n, device=DEVICE))\n",
    "\n",
    "# ===============================================================\n",
    "# 3. EVALUACIN OPTIMIZADA PARA 40GB VRAM\n",
    "# ===============================================================\n",
    "@torch.inference_mode()\n",
    "def calculate_metrics_gpu(model, edge_index, R_train_adj, R_target_adj, users_to_eval, novelty_tensor, K=20):\n",
    "    model.eval()\n",
    "    # Usamos float16 para ahorrar memoria en la matriz de scores\n",
    "    emb = model.get_embedding(edge_index).half()\n",
    "\n",
    "    # eval_batch_size reducido de 20k a 4k para evitar OOM en 40GB\n",
    "    eval_batch_size = 4000\n",
    "    recall_list, ndcg_list, novelty_list = [], [], []\n",
    "    recommended_items_all = set()\n",
    "\n",
    "    for i in range(0, len(users_to_eval), eval_batch_size):\n",
    "        batch_u = torch.tensor(users_to_eval[i:i + eval_batch_size], device=DEVICE)\n",
    "\n",
    "        # Matriz de scores en media precisi贸n\n",
    "        scores = torch.matmul(emb[batch_u], emb.t())\n",
    "\n",
    "        # Filtrado de 铆tems vistos (masking)\n",
    "        for idx, u_idx in enumerate(batch_u.cpu().numpy()):\n",
    "            seen = list(R_train_adj.get(u_idx, set()))\n",
    "            if seen: scores[idx, seen] = -1e4 # En float16 evitamos -1e10 para no desbordar\n",
    "            scores[idx, u_idx] = -1e4\n",
    "\n",
    "        _, top_indices = torch.topk(scores, K, dim=1)\n",
    "\n",
    "        # C谩lculo de Novelty\n",
    "        batch_nov = novelty_tensor[top_indices].mean(dim=1).cpu().numpy()\n",
    "        novelty_list.extend(batch_nov)\n",
    "\n",
    "        top_indices_cpu = top_indices.cpu().numpy()\n",
    "        recommended_items_all.update(top_indices_cpu.flatten())\n",
    "\n",
    "        for idx, u_idx in enumerate(batch_u.cpu().numpy()):\n",
    "            targets = list(R_target_adj.get(u_idx, set()))\n",
    "            if not targets: continue\n",
    "            preds = top_indices_cpu[idx]\n",
    "            hits = np.isin(preds, targets)\n",
    "            hit_count = np.sum(hits)\n",
    "\n",
    "            recall_list.append(hit_count / len(targets))\n",
    "            if hit_count > 0:\n",
    "                rank_pos = np.where(hits)[0] + 2\n",
    "                dcg = np.sum(1.0 / np.log2(rank_pos))\n",
    "                idcg = np.sum(1.0 / np.log2(np.arange(min(len(targets), K)) + 2))\n",
    "                ndcg_list.append(dcg / idcg)\n",
    "            else:\n",
    "                ndcg_list.append(0.0)\n",
    "\n",
    "        # Liberaci贸n expl铆cita de memoria del batch\n",
    "        del scores, batch_u, top_indices\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"recall\": np.mean(recall_list) if recall_list else 0,\n",
    "        \"ndcg\": np.mean(ndcg_list) if ndcg_list else 0,\n",
    "        \"novelty\": np.mean(novelty_list) if novelty_list else 0,\n",
    "        \"coverage\": len(recommended_items_all) / model.num_nodes\n",
    "    }\n",
    "\n",
    "# ===============================================================\n",
    "# 4. EXPERIMENTO PRINCIPAL\n",
    "# ===============================================================\n",
    "def run_optimized_experiment(df_raw, author_to_idx_raw):\n",
    "    author_to_idx = {str(k): int(v) for k, v in author_to_idx_raw.items()}\n",
    "    n_authors = len(author_to_idx)\n",
    "\n",
    "    df_filtered = preprocess_data(df_raw)\n",
    "    df_train, df_val, df_test = triple_loo_split(df_filtered)\n",
    "\n",
    "    edge_index_train = build_edge_index(df_train, author_to_idx)\n",
    "    adj_train = build_adj_dict(df_train, author_to_idx)\n",
    "    adj_val = build_adj_dict(df_val, author_to_idx)\n",
    "\n",
    "    train_keys = list(adj_train.keys())\n",
    "    adj_train_list = {k: list(v) for k, v in adj_train.items()}\n",
    "    novelty_train = get_item_popularity_tensor(df_train, author_to_idx, n_authors)\n",
    "\n",
    "    configs = [\n",
    "            {\n",
    "                'dim': 256,\n",
    "                'layers': 2,\n",
    "                'lr': 5e-4,\n",
    "                'reg': 1e-2,\n",
    "                'name': 'Anchor-Heavy'\n",
    "            },\n",
    "            {\n",
    "                'dim': 256,\n",
    "                'layers': 1,      # Solo vecinos directos para evitar ruido\n",
    "                'lr': 1e-3,\n",
    "                'reg': 1e-3,\n",
    "                'name': 'Shallow-Local'\n",
    "            },\n",
    "            {\n",
    "                'dim': 512,\n",
    "                'layers': 2,\n",
    "                'lr': 5e-4,\n",
    "                'reg': 1e-1,      # Regularizaci贸n extrema para \"matar\" el ruido\n",
    "                'name': 'Extreme-Reg'\n",
    "            }\n",
    "        ]\n",
    "    val_users = list(adj_val.keys())\n",
    "    sample_val = np.random.choice(val_users, min(20000, len(val_users)), replace=False)\n",
    "    best_ndcg, best_params = -1, {}\n",
    "\n",
    "    # Mantenemos el BATCH_SIZE de entrenamiento (131k) porque el entrenamiento\n",
    "    # consume mucha menos VRAM que la evaluaci贸n matricial completa.\n",
    "    BATCH_SIZE = 131072\n",
    "\n",
    "    for i, conf in enumerate(configs):\n",
    "        print(f\"\\n Iniciando Config {i+1} [{conf['name']}]: Dim={conf['dim']}, Layers={conf['layers']}\")\n",
    "        model = LightGCN(num_nodes=n_authors, embedding_dim=conf['dim'], num_layers=conf['layers']).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=conf['lr'])\n",
    "\n",
    "        for epoch in range(1, 61):\n",
    "            model.train()\n",
    "            u, p, n = sample_batch_fast(adj_train_list, train_keys, n_authors, BATCH_SIZE)\n",
    "            optimizer.zero_grad()\n",
    "            emb = model.get_embedding(edge_index_train)\n",
    "            loss = model.recommendation_loss((emb[u]*emb[p]).sum(dim=1), (emb[u]*emb[n]).sum(dim=1), torch.cat([u,p,n]).unique(), lambda_reg=conf['reg'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        m = calculate_metrics_gpu(model, edge_index_train, adj_train, adj_val, sample_val, novelty_train)\n",
    "        print(f\"   Val Final -> Recall: {m['recall']:.4f} | NDCG: {m['ndcg']:.4f} | Novelty: {m['novelty']:.4f}\")\n",
    "\n",
    "        if m['ndcg'] > best_ndcg:\n",
    "            best_ndcg = m['ndcg']; best_params = conf\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # --- ENTRENAMIENTO FINAL ---\n",
    "    print(f\"\\n GANADOR: {best_params['name']} con NDCG: {best_ndcg:.4f}\")\n",
    "    df_full_train = pd.concat([df_train, df_val])\n",
    "    edge_index_full = build_edge_index(df_full_train, author_to_idx)\n",
    "    adj_full = build_adj_dict(df_full_train, author_to_idx)\n",
    "    adj_full_list = {k: list(v) for k, v in adj_full.items()}\n",
    "    full_keys = list(adj_full.keys())\n",
    "    novelty_full = get_item_popularity_tensor(df_full_train, author_to_idx, n_authors)\n",
    "\n",
    "    final_model = LightGCN(num_nodes=n_authors, embedding_dim=best_params['dim'], num_layers=best_params['layers']).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "    for epoch in tqdm(range(1, 201), desc=\"Training Final Model\"):\n",
    "        final_model.train()\n",
    "        u, p, n = sample_batch_fast(adj_full_list, full_keys, n_authors, BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        emb = final_model.get_embedding(edge_index_full)\n",
    "        loss = final_model.recommendation_loss((emb[u]*emb[p]).sum(dim=1), (emb[u]*emb[n]).sum(dim=1), torch.cat([u,p,n]).unique(), lambda_reg=best_params['reg'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # --- EVALUACIN TEST ---\n",
    "    adj_test = build_adj_dict(df_test, author_to_idx)\n",
    "    test_users = list(adj_test.keys())\n",
    "    res = calculate_metrics_gpu(final_model, edge_index_full, adj_full, adj_test, test_users, novelty_full)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RESULTADO FINAL LIGHTGCN EN TEST\")\n",
    "    print(f\"Recall@20: {res['recall']:.6f}\")\n",
    "    print(f\"NDCG@20:   {res['ndcg']:.6f}\")\n",
    "    print(f\"Novelty:   {res['novelty']:.6f}\")\n",
    "    print(f\"Coverage:  {res['coverage']:.2%}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    torch.save(final_model.state_dict(), f\"{SAVE_DIR}/lightgcn_final.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_raw = pd.read_pickle(\"data/df_pairs_unique.pkl\")\n",
    "    author_to_idx = np.load(\"data/author_to_idx.npy\", allow_pickle=True).item()\n",
    "    run_optimized_experiment(df_raw, author_to_idx)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
