{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669527b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.preprocessing import normalize\n",
    "import gc\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CARGAR DATOS\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"CARGANDO DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pairs_unique = pd.read_pickle(\"data/df_pairs_unique.pkl\")\n",
    "author2idx = np.load(\"data/author_to_idx.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Filtrar autores con >= 2 colaboraciones\n",
    "author_counts = df_pairs_unique[['pair_min', 'pair_max']].stack().value_counts()\n",
    "eligible_authors = set(author_counts[author_counts >= 2].index)\n",
    "\n",
    "df_filtered = df_pairs_unique[\n",
    "    df_pairs_unique['pair_min'].isin(eligible_authors) &\n",
    "    df_pairs_unique['pair_max'].isin(eligible_authors)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CARGAR MODELO CONTENT-BASED (OPTIMIZADO)\n",
    "# =============================================================================\n",
    "models_dir = \"data\"\n",
    "\n",
    "# Carga y conversión inmediata a float32 para ahorrar 50% de RAM\n",
    "print(\"Cargando matriz de conceptos...\")\n",
    "author_matrix = load_npz(os.path.join(models_dir, 'author_concept_matrix.npz')).astype(np.float32)\n",
    "\n",
    "# NORMALIZACIÓN PREVIA: Crucial para usar dot product como similitud de coseno\n",
    "print(\"Normalizando matriz para similitud de coseno rápida...\")\n",
    "author_matrix = normalize(author_matrix, norm='l2', axis=1)\n",
    "\n",
    "cb_author_ids = np.load(os.path.join(models_dir, 'cb_author_ids.npy'), allow_pickle=True)\n",
    "cb_author_to_idx = {aid: i for i, aid in enumerate(cb_author_ids)}\n",
    "\n",
    "try:\n",
    "    author_work_counts = np.load(os.path.join(models_dir, 'cb_author_work_counts.npy')).astype(np.float32)\n",
    "except FileNotFoundError:\n",
    "    author_work_counts = np.ones(len(cb_author_ids), dtype=np.float32)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FUNCIONES DE MÉTRICAS Y PROCESAMIENTO POR LOTES\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_metrics_batched(target_author_ids, gt, author_matrix, work_counts, C, topk=20, batch_size=500):\n",
    "    \"\"\"\n",
    "    Calcula Recall y NDCG procesando autores en bloques para no saturar la RAM.\n",
    "    \"\"\"\n",
    "    total_recall = 0.0\n",
    "    total_ndcg = 0.0\n",
    "    valid_users = 0\n",
    "\n",
    "    # Mapear IDs de autores a sus índices en la matriz\n",
    "    target_indices = [cb_author_to_idx[aid] for aid in target_author_ids if aid in cb_author_to_idx]\n",
    "\n",
    "    for i in tqdm(range(0, len(target_indices), batch_size), desc=f\"Evaluando C={C}\"):\n",
    "        batch_idx = target_indices[i : i + batch_size]\n",
    "\n",
    "        # Similitud de coseno masiva: (Batch_Size x Total_Autores)\n",
    "        # Esto es lo que causaba el crash, ahora está controlado por batch_size\n",
    "        sims_batch = author_matrix[batch_idx].dot(author_matrix.T).toarray()\n",
    "\n",
    "        for j, u_idx in enumerate(batch_idx):\n",
    "            u_id = cb_author_ids[u_idx]\n",
    "            if u_id not in gt: continue\n",
    "\n",
    "            rel_set = gt[u_id]\n",
    "            sims = sims_batch[j]\n",
    "\n",
    "            # Bayesian Smoothing Vectorizado\n",
    "            m = sims.mean()\n",
    "            sims = (C * m + work_counts * sims) / (C + work_counts)\n",
    "\n",
    "            # Excluir al mismo autor\n",
    "            sims[u_idx] = -1.0\n",
    "\n",
    "            # Obtener Top-K usando argpartition (O(n) vs O(n log n))\n",
    "            top_indices = np.argpartition(-sims, topk)[:topk]\n",
    "            # Ordenar solo esos 20\n",
    "            top_indices = top_indices[np.argsort(-sims[top_indices])]\n",
    "\n",
    "            recs = cb_author_ids[top_indices]\n",
    "            hits = [1 if r in rel_set else 0 for r in recs]\n",
    "\n",
    "            # Acumular métricas\n",
    "            total_recall += sum(hits) / len(rel_set)\n",
    "\n",
    "            dcg = sum(h / np.log2(idx + 2) for idx, h in enumerate(hits))\n",
    "            idcg = sum(1 / np.log2(idx + 2) for idx in range(min(len(rel_set), topk)))\n",
    "            total_ndcg += (dcg / idcg) if idcg > 0 else 0\n",
    "\n",
    "            valid_users += 1\n",
    "\n",
    "        # Liberar memoria del batch explícitamente\n",
    "        del sims_batch\n",
    "        if i % 5000 == 0: gc.collect()\n",
    "\n",
    "    return (total_recall / valid_users, total_ndcg / valid_users) if valid_users > 0 else (0, 0)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. SPLIT Y PREPARACIÓN\n",
    "# =============================================================================\n",
    "def build_gt(df):\n",
    "    gt = defaultdict(set)\n",
    "    for r in df.itertuples():\n",
    "        gt[r.pair_min].add(r.pair_max)\n",
    "        gt[r.pair_max].add(r.pair_min)\n",
    "    return gt\n",
    "\n",
    "def triple_loo_split(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    adj = defaultdict(list)\n",
    "    for i, r in enumerate(df.itertuples()):\n",
    "        adj[r.pair_min].append(i)\n",
    "        adj[r.pair_max].append(i)\n",
    "\n",
    "    test_idx, val_idx, used = set(), set(), set()\n",
    "    for _, idxs in adj.items():\n",
    "        idxs = [i for i in idxs if i not in used]\n",
    "        if len(idxs) >= 3:\n",
    "            rng.shuffle(idxs)\n",
    "            test_idx.add(idxs[0]); val_idx.add(idxs[1])\n",
    "            used.update(idxs[:2])\n",
    "        elif len(idxs) == 2:\n",
    "            test_idx.add(idxs[0]); used.add(idxs[0])\n",
    "\n",
    "    all_idx = set(range(len(df)))\n",
    "    train_idx = sorted(all_idx - test_idx - val_idx)\n",
    "    return (df.iloc[train_idx].reset_index(drop=True),\n",
    "            df.iloc[list(val_idx)].reset_index(drop=True),\n",
    "            df.iloc[list(test_idx)].reset_index(drop=True))\n",
    "\n",
    "print(\"\\nGenerando splits...\")\n",
    "df_train, df_val, df_test = triple_loo_split(df_filtered)\n",
    "gt_val = build_gt(df_val)\n",
    "gt_test = build_gt(df_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TUNING (20,000 autores sample)\n",
    "# =============================================================================\n",
    "#rng = np.random.default_rng(42)\n",
    "#val_authors_all = [a for a in gt_val.keys() if a in cb_author_to_idx]\n",
    "#val_sample = rng.choice(val_authors_all, size=min(20000, len(val_authors_all)), replace=False)\n",
    "\n",
    "#Cs = [1, 10, 20, 50, 100]\n",
    "#best_C, best_ndcg = None, -1\n",
    "\n",
    "#print(f\"\\n--- TUNING EN VALIDACIÓN ({len(val_sample)} autores) ---\")\n",
    "#for C in Cs:\n",
    "#    rec, ndcg = calculate_metrics_batched(val_sample, gt_val, author_matrix, author_work_counts, C)\n",
    "#    print(f\"C={C:5.1f} | Recall@20={rec:.4f} | NDCG@20={ndcg:.4f}\")\n",
    "#    if ndcg > best_ndcg:\n",
    "#        best_ndcg, best_C = ndcg, C\n",
    "\n",
    "# =============================================================================\n",
    "# 6. EVALUACIÓN FINAL FULL (518k autores)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"EVALUACIÓN FINAL EN TEST (FULL: {len(gt_test)} autores)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_C = 1\n",
    "\n",
    "# Limpiar memoria antes del proceso largo\n",
    "gc.collect()\n",
    "\n",
    "test_authors_full = list(gt_test.keys())\n",
    "final_rec, final_ndcg = calculate_metrics_batched(\n",
    "    test_authors_full, gt_test, author_matrix, author_work_counts, best_C, batch_size=8000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"#\" * 40)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(f\"Mejor C: {best_C}\")\n",
    "print(f\"Recall@20: {final_rec:.4f}\")\n",
    "print(f\"NDCG@20 : {final_ndcg:.4f}\")\n",
    "print(\"#\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
